from CrossValidation import CrossValidation
import numpy as np
import FileFunctions as ff
from Model import Model
from matplotlib import pyplot as plt
import json
# ================ DATA GET ========================

# -----DATA FOR ELECTRIC CONSUMPTION ESTIMATION-----
energy_X = np.genfromtxt("data/tp1_ej2_training.csv",
                         delimiter=",", dtype=float)[:, :-2]
energy_Y = np.genfromtxt("data/tp1_ej2_training.csv",
                         delimiter=",", dtype=float)[:, -2:]

# -----VARIBLES-----
datasetName = "energy"
X = energy_X
Z = energy_Y

X = (X - np.min(X, axis=0))/(np.max(X, axis=0) -
                             np.min(X, axis=0))  # datos de entrada normalizados
Z = (Z - np.min(Z, axis=0))/(np.max(Z, axis=0) -
                             np.min(Z, axis=0))  # datos de salida normalizados


# -------------------------------------


exp_name = f'energy_one_inner_layer'
arrayFunc = ["sigmoid", "sigmoid"]
errors = {}
for i in range(2, 10):
    name_template = "results-energy/results_#exp_name#"
    S = [X.shape[1], i, Z.shape[1]]
    label_string = f"[{S[0]},{S[1]},{S[2]}]"
    model = Model(S, arrayFunc,
                  learningRate=0.3)

    iters, error, learning = model.train(X, Z)

    # exp_info = [model.S, model.activationFuncArray,
    #             model.learningRate, iters, model.maxIter, error]
    # run_number = ff.store(exp_name, exp_info)

    name = name_template.replace("#exp_name#", exp_name)
    name = name + f"-{i}-"
    # name = name.replace("#run_number#", run_number)
    plot_name = name + f"{label_string}.png"
    # csv_name = name + ".csv"
    learning = np.mean(learning, axis=1).tolist()
    errors[label_string] = learning
    plt.plot(learning,
             label=f"Label = {label_string}")
    plt.title("Error evolution")
    plt.ylabel("Estimation error")
    plt.ylim((0.035, .07))
    plt.xlabel("Epoch")
    plt.legend()

    plt.savefig(plot_name)

with open(f"{name}.json", "w") as write_file:
    json.dump(errors, write_file)

# -----------------------------------------------------------------------


# errors = {}
# data = [0.107878720950283, 0.08342133167923016, 0.07500917640434238, 0.06997020930954456, 0.06682266991745808, 0.06523050050906318, 0.0644769674777613, 0.06410292561817033, 0.06386447442902113, 0.0636415328215602, 0.06340012859550184, 0.06313531374477488, 0.0628538155796799, 0.06256534091811682, 0.06228073689703474, 0.062002504051598135, 0.06173855654568092, 0.0614899847585641, 0.06124856310997093, 0.06102239836069759, 0.0608111045721008, 0.06061295155750191, 0.060427691081021036, 0.0602521668969236, 0.06008652273197811, 0.05993046947364718, 0.05978913807359404, 0.05966043442405024, 0.059539714892965535, 0.059425197769171406, 0.05931726747863965, 0.05921587054097868, 0.05912311289136975, 0.05903720657414378, 0.058958036094318156, 0.05888326584188648, 0.05881169839740491, 0.05874352304748332, 0.05867970017696366, 0.05862079669845098, 0.05856470248420155, 0.05851276628559375, 0.058463299444281684, 0.058416402718614716, 0.05837166953646774, 0.05832920035709923, 0.058288690575412375, 0.058250904332710324, 0.0582153223809868, 0.058181186966231785, 0.05814803197513943, 0.05811591676059696, 0.05808534642552547, 0.05805702450253343, 0.05802968375626688, 0.05800306186598467, 0.057977160224497115, 0.05795177981598666, 0.057926786732431824, 0.05790225743281173, 0.05787806179318556, 0.05785412785868329, 0.0578304641906667, 0.05780718218510293, 0.05778411597022591, 0.05776148371727145, 0.05773897064368493, 0.05771666458906989, 0.05769448923721816, 0.05767242057095171, 0.05765058513724098, 0.05762903332882468, 0.057607657544871946, 0.05758640540596388, 0.05756500322105362, 0.057543428206065034, 0.057521657611751986, 0.05749966864975674, 0.057477461281984485, 0.05745515997364345, 0.05743257092691963, 0.05741000497243241, 0.05738731546080382, 0.0573646510864919, 0.05734182166232438, 0.05731856465288784, 0.057295072551831576, 0.057271517336455836, 0.057247493611283104, 0.057223284536150665, 0.05719855016396856, 0.05717360249051837, 0.057148108749103414, 0.05712216088015466, 0.05709560721629976, 0.05706835723446729, 0.05704038057737224, 0.05701166862862115, 0.05698220345143268, 0.056952321976299305, 0.056921614897582254, 0.05689097925545625, 0.05685992581989635, 0.056827927587110354, 0.05679499435301734, 0.05676112890137851, 0.05672621671614993, 0.05669069943675059, 0.05665430431439562, 0.05661682192939046, 0.056578171752929396, 0.056538324066274834, 0.056497289922452947, 0.056455243660807376, 0.05641205358478443, 0.056367573337852535, 0.056321907458706494, 0.056275316305637216, 0.05622736286454911, 0.0561780236681718, 0.05612781228166232, 0.05607624040759806, 0.056023819674412015, 0.05597058712296252, 0.055916433329377055, 0.05586216829109123, 0.0558079268578814, 0.05575240915955572, 0.055695662665243714, 0.05563794566460381, 0.05557879938084048, 0.055518146737309135, 0.05545665008447885, 0.05539471126918549, 0.05533216053246522, 0.05526809394078175, 0.055202861764958755, 0.05513665752421554, 0.05506990755711816, 0.05500264802460991, 0.05493389215227158, 0.054863683285946374, 0.05479201749403841, 0.0547188653279952, 0.0546443870879333, 0.05456894428426074, 0.054492729429355485, 0.05441512456735456, 0.05433594663292955, 0.05425598402368502, 0.054174964405994175, 0.05409393663222719, 0.05401343630208178, 0.05393221024093994, 0.05385043982264151, 0.05376805387404892, 0.05368492910371708, 0.05360056952393828, 0.0535154032928415, 0.05342971452869516, 0.05334315265264951, 0.05325550095553759, 0.0531677994477745, 0.05307901862619091, 0.0529892550455863, 0.05289998324677496, 0.05280953736984584, 0.05271932593884056, 0.05262841467358481, 0.05253633139900755, 0.05244433419967629, 0.0523531420067023, 0.05226120783941336, 0.05216881165460528, 0.05207802324419095, 0.05198982337353511, 0.051901429277825, 0.05181233214560034, 0.051723561764377105, 0.05163518960491373, 0.05154734757871457, 0.05145909873590311, 0.05137142551698519, 0.05128411673583509, 0.05119698810443769, 0.05111178726788247, 0.05102711907263623, 0.05094316594196108, 0.050858850022064236, 0.05077438202569101, 0.05068959868702568, 0.050605385845955975, 0.05052160361522454, 0.05043847458400425, 0.05035463861239069, 0.05027041767952027, 0.050186283481981155, 0.05010254976161251, 0.05001873431051829, 0.049934326223721216, 0.049848988360910625, 0.049763779633239094, 0.04967819928951031, 0.0495932421669273, 0.04950925990406073, 0.049425579747029255, 0.04934134290604404, 0.04925666462682036, 0.04917254583626097, 0.049088309722689266, 0.04900460662671053, 0.04892172870343828, 0.04883930821457549, 0.0487558035597065, 0.04867211709230085, 0.04858839706375602, 0.04850547756847849, 0.04842454470731754, 0.04834506879571095, 0.04826608730751494, 0.048186387283825495, 0.048106502506573404, 0.04802662830983566, 0.04794700511920273, 0.047869056496341295, 0.04779293176646179, 0.04771867835186759, 0.047645083960659994, 0.0475727945663459, 0.047502003441801666, 0.04743134725664139, 0.04736051164442405, 0.04729040639701751, 0.04722203854561198, 0.0471558449792262, 0.047089669525345476, 0.04702234789256969, 0.04695441321731417, 0.04688632845619428, 0.04681980069127012, 0.04675382557110702, 0.04668825617011654, 0.046623398680027836, 0.04655916550281673, 0.046495892114626666, 0.04643235646204457, 0.046368226756482535, 0.046307321434031185, 0.04624655004690084, 0.04618558972697056,
#         0.046125262210136866, 0.046065579590330384, 0.04600590752903842, 0.04594659703913809, 0.04588742105859153, 0.04582898097926206, 0.045771305988707225, 0.04571316682986831, 0.04565444147451195, 0.04559651317119691, 0.04553830142316691, 0.04547916054635974, 0.045419658981779805, 0.04535998059579874, 0.04529973709779693, 0.04523910766582267, 0.04517764736791749, 0.045116612988110205, 0.045055299280636604, 0.04499314491063272, 0.044930925109022885, 0.044868899241306624, 0.04480695562414372, 0.044744546065255165, 0.04468184926970149, 0.044618807108519655, 0.04455505486801689, 0.04449089008960726, 0.04442744588555926, 0.04436497309008888, 0.04430270729865425, 0.04424034114769619, 0.044177917823896476, 0.044115493698932945, 0.044053349818223925, 0.04399144411155618, 0.04392920259475116, 0.043866238500458156, 0.04380256564241506, 0.04373819816536702, 0.04367315054076139, 0.043607452010529804, 0.0435414972661796, 0.04347498121151399, 0.04340824977361159, 0.043342055673135704, 0.04327572569044166, 0.04320902002804057, 0.04314216768602793, 0.04307577007261377, 0.043010184539850824, 0.04294565322440583, 0.04288107094777649, 0.04281599308442825, 0.04275114199827479, 0.04268601693975045, 0.04262097300412217, 0.04255633345208103, 0.04249135566154751, 0.042426176568645266, 0.04236080625695867, 0.04229516383870439, 0.04222961111950953, 0.04216420042628301, 0.04209900914297402, 0.042034235124670284, 0.04197021286870963, 0.041906071188281296, 0.04184218244482591, 0.041778454361077896, 0.041715734580584724, 0.04165354576326982, 0.041591453269359355, 0.04152984987949099, 0.04146831305959673, 0.04140765661580575, 0.04134741320498879, 0.04128738947244888, 0.041227640898951506, 0.04116797027620304, 0.041108456096305285, 0.04104923508403081, 0.04099032329394701, 0.040932371357824404, 0.04087491520516585, 0.04081770371286565, 0.040761100184059436, 0.040705743556232604, 0.040651145563738486, 0.04059709456720512, 0.040543795326923676, 0.04049088761035292, 0.04043819556329196, 0.04038545636667216, 0.04033281732889996, 0.04028065204605161, 0.04022898092705963, 0.0401776808114754, 0.040127724348334726, 0.04007799685726947, 0.04002900164068092, 0.03998002077627258, 0.0399312348503706, 0.03988276125563843, 0.03983481018934994, 0.039787046281170926, 0.039739925931067024, 0.03969367468279304, 0.03964779814806213, 0.039602207790627675, 0.03955665320899196, 0.03951174525290478, 0.03946716358128281, 0.039422609777706744, 0.039378450065825826, 0.03933520118840972, 0.03929248019785227, 0.03925026365119459, 0.03920845937777208, 0.03916705165120285, 0.03912606604608932, 0.03908525199031089, 0.03904501722852371, 0.03900506956814237, 0.038965223088171214, 0.038926446400025, 0.038888077103498675, 0.0388498614443068, 0.03881188109453109, 0.038774094017674975, 0.038736545799790446, 0.03869926745487966, 0.0386626583382558, 0.0386262392673551, 0.0385899953990152, 0.03855390163615772, 0.03851866761391291, 0.03848384997189927, 0.038449408903843414, 0.03841543892905453, 0.038381738885474, 0.038348606128690406, 0.03831611864264925, 0.03828383912373148, 0.038251691255041395, 0.03821957029730194, 0.03818747700339026, 0.03815584050025223, 0.03812448885965295, 0.03809349897270069, 0.038062561093416186, 0.03803164519077805, 0.03800075175020578, 0.03796999975609652, 0.03793946205176209, 0.03790945888662055, 0.0378795523836558, 0.037849867539895235, 0.03782058047961158, 0.03779132141291153, 0.03776216009294384, 0.03773326758195783, 0.03770452518638916, 0.03767598247136239, 0.03764760003234412, 0.03761931958440683, 0.03759125941952141, 0.03756360325057548, 0.03753618619100421, 0.03750888183757807, 0.0374815883055, 0.03745440006029026, 0.03742766270322062, 0.03740097754495761, 0.03737438913246445, 0.037348064193580194, 0.03732184132800176, 0.03729652863525196, 0.037271906553121556, 0.03724737279244908, 0.037222971422217505, 0.037198680577490775, 0.03717443294487757, 0.0371502031727312, 0.037126227818846745, 0.037102335713223675, 0.0370786811983395, 0.03705501401438905, 0.037031406325745794, 0.03700779159481599, 0.0369842852398782, 0.036960829724133504, 0.03693744597018371, 0.03691411812727215, 0.03689093860082868, 0.03686784563652687, 0.036845027398202584, 0.03682260369588089, 0.036800701397143885, 0.03677877163765368, 0.03675681412466046, 0.036734928753971705, 0.03671358826390711, 0.03669262773199121, 0.03667215734407626, 0.03665203333250516, 0.036631868732560496, 0.03661166347633474, 0.03659144872081846, 0.03657142713457702, 0.03655144926582371, 0.03653142588297248, 0.03651150986504287, 0.03649165141301031, 0.03647192821329466, 0.03645218947714607, 0.03643250530785325, 0.03641297345051889, 0.03639354809100716, 0.03637425263271194, 0.036355030402353106, 0.036335926476155384, 0.036316800735858745, 0.03629761369327523, 0.03627836507937482, 0.036259164190817025, 0.03624011729653991, 0.03622103734937126, 0.03620190245316358, 0.03618270051689819, 0.036163431270883784, 0.036144094446620846, 0.0361246897769762, 0.03610530516786223, 0.036085993674600456, 0.036066786603381425, 0.03604752865932084, 0.0360282596457189, 0.03600918568036464, 0.03599003940231609, 0.03597082743917847, 0.03595168298558087, 0.03593252375168554, 0.035913591358327585, 0.03589460840826809, 0.03587554985294324, 0.03585643415121567, 0.035837378907945194, 0.03581824613575006, 0.03579905274078054]
# plt.plot(data,
#          label="Label = [8,8,2]")
# plt.title("Error evolution")
# plt.ylabel("Estimation error")
# plt.ylim((0.025, .07))
# plt.xlabel("Epoch")
# plt.legend()

# f = open('results-energy/results_energy_two_inner_layer-7-.json')

# data = json.load(f)

# for i in range(2, 8):
#     plt.plot(data[f"[8,8,{i},2]"],
#              label=f"Label = [8,8,{i},2]")
#     plt.title("Error evolution")
#     plt.ylabel("Estimation error")
#     plt.ylim((0.025, .07))
#     plt.xlabel("Epoch")
#     plt.legend()


# exp_name = f'energy_two_inner_layer'
# arrayFunc = ["sigmoid", "sigmoid", "sigmoid"]
# for i in range(8, 11):
#     name_template = "results-energy/results_#exp_name#"
#     S = [X.shape[1], 8, i, Z.shape[1]]
#     label_string = f"[{S[0]},{S[1]},{S[2]},{S[3]}]"
#     model = Model(S, arrayFunc,
#                   learningRate=0.3)

#     iters, error, learning = model.train(X, Z)

#     learning = np.mean(learning, axis=1).tolist()
#     errors[label_string] = learning
#     name = name_template.replace("#exp_name#", exp_name)
#     name = name + f"-{i}-"

#     plot_name = name + f"{label_string}.png"
#     plt.plot(learning, label=f"Label = [8,8,{i},2]")
#     plt.title("Error evolution")
#     plt.ylabel("Estimation error")
#     plt.ylim((0.025, .07))
#     plt.xlabel("Epoch")
#     plt.legend()

#     plt.savefig(plot_name)

# with open(f"{name}.json", "w") as write_file:
#     json.dump(errors, write_file)

# -----------------------------------------------------------------------

# errors = {}
# exp_name = f'energy_best_inner_layer'
# for i in range(7, 9):
#     name_template = "results-energy/results_#exp_name#"
#     S = [X.shape[1], 8, i, Z.shape[1]]
#     label_string = f"[{S[0]},{S[1]},{S[2]},{S[3]}]"
#     model = Model(S,
#                   learningRate=0.3)

#     iters, error, learning = model.train(X, Z)

#     learning = np.mean(learning, axis=1).tolist()
#     errors[label_string] = learning
#     name = name_template.replace("#exp_name#", exp_name)
#     name = name + f"-{i}-"

#     plot_name = name + f"{label_string}.png"
#     plt.plot(learning, label=f"Label = [8,8,{i},2]")
#     plt.title("Error evolution")
#     plt.ylabel("Estimation error")
#     plt.ylim((0.025, .07))
#     plt.xlabel("Epoch")
#     plt.legend()

#     plt.savefig(plot_name)

# name_template = "results-energy/results_#exp_name#"
# S = [X.shape[1], 8,  Z.shape[1]]
# label_string = f"[{S[0]},{S[1]},{S[2]}]"
# model = Model(S,
#               learningRate=0.3)

# iters, error, learning = model.train(X, Z)

# learning = np.mean(learning, axis=1).tolist()
# errors[label_string] = learning
# name = name_template.replace("#exp_name#", exp_name)
# name = name + "-8-"

# plot_name = name + f"{label_string}.png"
# plt.plot(learning, label="Label = [8,8,2]")
# plt.title("Error evolution")
# plt.ylabel("Estimation error")
# plt.ylim((0.001, .07))
# plt.xlabel("Epoch")
# plt.legend()

# plt.savefig(plot_name)

# with open(f"{name}.json", "w") as write_file:
#     json.dump(errors, write_file)


# ----------------------------------------

f = open('results-energy/results_energy_lr-0.35-.json')

data = json.load(f)

errors = {}
exp_name = f'energy_lr'
arrayFunc = ["sigmoid", "sigmoid", "sigmoid"]
for lr in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]:
    name_template = "results-energy/results_#exp_name#"
    S = [X.shape[1], 8, 7, Z.shape[1]]
    label_string = f"[{S[0]},{S[1]},{S[2]},{S[3]}]"
    model = Model(S, arrayFunc,
                  learningRate=0.3)

    iters, error, learning = model.train(X, Z)

    learning = np.mean(learning, axis=1).tolist()
    errors[lr] = learning
    name = name_template.replace("#exp_name#", exp_name)
    name = name + f"-{lr}-"

    plot_name = name + f"{label_string}.png"
    plt.plot(data[f"{lr}"], label=f"lr = {lr}")
    plt.title("Error evolution with [8,8,7,2]")
    plt.ylabel("Estimation error")
    plt.ylim((0.02, .07))
    plt.xlabel("Epoch")
    plt.legend()

plt.savefig(plot_name)

# with open(f"{name}.json", "w") as write_file:
#     json.dump(errors, write_file)
